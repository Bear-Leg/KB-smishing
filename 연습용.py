# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1EOywxkl2G5ZLSPeKzHHPu7cdDAl3dyhI
"""

pip install ktrain

import ktrain
from ktrain import text

"""# 사전 사용 패키지 다운로드"""

import pandas as pd # 데이터 전처리
import numpy as np # 데이터 전처리
import random #데이터 전처리
from pandas import DataFrame #데이터 전처리
from collections import Counter #데이터 전처리

from tqdm import tqdm #시간 측정용

from sklearn.feature_extraction.text import CountVectorizer # model setting
from sklearn.model_selection import train_test_split  # model setting

from sklearn.naive_bayes import MultinomialNB  # model 관련
from sklearn.metrics import roc_auc_score  # model 성능 확인

"""# 대회 데이터 다운로드 및 사전 확인

google colab에서는 google drive 안에 있는 파일들을 바로 불러와서 사용이 가능하다.
"""

from google.colab import drive

drive.mount('/content/gdrive')

"""일반적으로 들어가지는 구글드라이브에  14th data라는 폴더를 만들고 그 안에 경진대회의 데이터를 이동시켜놓았다"""

!ls "/content/gdrive/My Drive/스팸문자공모전/14th data" #현재 drive 경로 안의 파일 or 폴더 표시

cd /content/gdrive/My Drive/스팸문자공모전/14th data

train = pd.read_csv("train.csv") #해당 14th data의 csv 파일 중 train.csv 불러오기

train.head(2)

test = pd.read_csv("public_test.csv")

test.head(2)

train.shape, test.shape

from sklearn.model_selection import StratifiedShuffleSplit
split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)

for train_index, test_index in split.split(train_xx, train_yy):
    X_train, X_test = train_xx.iloc[train_index], train_xx.iloc[test_index]
    Y_train, Y_test = train_yy.iloc[train_index], train_yy.iloc[test_index]
print(len(X_train))
print(Y_train["smishing"].value_counts() / len(X_train))

(x_train,  y_train), (x_test, y_test), preproc = text.texts_from_array(x_train=X_train, y_train=Y_train,
                                                                       x_test=X_test, y_test=Y_test,
                                                                       preprocess_mode='standard',
                                                                       ngram_range=1, 
                                                                       maxlen=350, 
                                                                       max_features=35000)

"""# Mecab 다운로드 및 사용

현재 다양한 자연어 처리 패키지 중에서 mecab는 윈도우에서는 설치가 힘든 패키지 중 하나이다.

이를 극복하기 위해, colab에서 mecab를 설치하고 활용할 수 있도록 colab 파일을 공유하고자 한다.
"""

! git clone https://github.com/SOMJANG/Mecab-ko-for-Google-Colab.git

ls

cd Mecab-ko-for-Google-Colab/

ls

! bash install_mecab-ko_on_colab190912.sh

import konlpy
from konlpy.tag import Mecab

tokenizer = Mecab() # setting tokenizer using Mecab()

train_doc = [ ( tokenizer.pos(x), y ) for x, y in tqdm( zip( train_xx['text'], train_yyy['smishing'] ) )  ] # Mecab를 활용하여 text를 토큰화 시킴
test_doc = [ ( tokenizer.pos(x), y ) for x, y in tqdm( zip( test_xx['text'], test_yyy['smishing'] ) )  ]

stopwords = ['XXX', '.', '을', '를', '이', '가', '-', '(', ')', ':', '!', '?', ')-', '.-', 'ㅡ', 'XXXXXX', '..', '.(', '은', '는'] #필요없는 단어 리스트

def get_couple(_words): #필요없는 단어들 없애는 함수
    global stopwords
    _words = [x for x in _words if x[0] not in stopwords]
    l = len(_words)
    for i in range(l-1):
        yield _words[i][0], _words[i+1][0]

"""필요없는 단어들을 제거하면서 동시에 모형에 사용하기 위한, X_train,test, Y_train,test를 생성한다"""

X_train, Y_train = [], []
for lwords in train_doc:
    Y_train.append(lwords[1])
    
    temp = []
    for x, y in get_couple(lwords[0]):
        temp.append("{}.{}".format(x, y))
    
    X_train.append(" ".join(temp))

X_train[0:1]

X_test = []
for lwords in test_doc:
    
    temp = []
    for x, y in get_couple(lwords[0]):
        temp.append("{}.{}".format(x, y))
    
    X_test.append(" ".join(temp))

